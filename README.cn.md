[English](./README.md) | ä¸­æ–‡

# SmartPrompt

[![Gem Version](https://badge.fury.io/rb/smart_prompt.svg)](https://badge.fury.io/rb/smart_prompt)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

SmartPrompt æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ Ruby gemï¼Œæä¾›äº†ä¼˜é›…çš„é¢†åŸŸç‰¹å®šè¯­è¨€ï¼ˆDSLï¼‰ï¼Œç”¨äºæ„å»ºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½åº”ç”¨ç¨‹åºã€‚å®ƒä½¿ Ruby ç¨‹åºèƒ½å¤Ÿæ— ç¼åœ°ä¸å„ç§ LLM æœåŠ¡æä¾›å•†äº¤äº’ï¼ŒåŒæ—¶ä¿æŒæ¸…æ™°ã€å¯ç»„åˆå’Œé«˜åº¦å¯å®šåˆ¶çš„ä»£ç æ¶æ„ã€‚

## ğŸš€ æ ¸å¿ƒç‰¹æ€§

### å¤š LLM æ”¯æŒ
- **OpenAI API å…¼å®¹**: å®Œå…¨æ”¯æŒ OpenAI GPT æ¨¡å‹å’Œå…¼å®¹çš„ API
- **Llama.cpp é›†æˆ**: ç›´æ¥é›†æˆæœ¬åœ° Llama.cpp æœåŠ¡å™¨  
- **å¯æ‰©å±•é€‚é…å™¨**: æ˜“äºæ‰©å±•çš„é€‚é…å™¨ç³»ç»Ÿï¼Œæ”¯æŒæ–°çš„ LLM æä¾›å•†
- **ç»Ÿä¸€æ¥å£**: æ— è®ºåº•å±‚ LLM æä¾›å•†å¦‚ä½•ï¼Œéƒ½ä½¿ç”¨ç›¸åŒçš„ API

### çµæ´»æ¶æ„
- **åŸºäº Worker çš„ä»»åŠ¡**: ä¸ºç‰¹å®š AI ä»»åŠ¡å®šä¹‰å¯é‡ç”¨çš„ Worker
- **æ¨¡æ¿ç³»ç»Ÿ**: åŸºäº ERB çš„æç¤ºè¯æ¨¡æ¿ï¼Œæ”¯æŒå‚æ•°æ³¨å…¥
- **å¯¹è¯ç®¡ç†**: å†…ç½®å¯¹è¯å†å²å’Œä¸Šä¸‹æ–‡ç®¡ç†
- **æµå¼æ”¯æŒ**: å®æ—¶å“åº”æµï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ

### é«˜çº§åŠŸèƒ½
- **å·¥å…·è°ƒç”¨**: åŸç”Ÿæ”¯æŒå‡½æ•°è°ƒç”¨å’Œå·¥å…·é›†æˆ
- **é‡è¯•é€»è¾‘**: å¼ºå¤§çš„é”™è¯¯å¤„ç†æœºåˆ¶ï¼Œæ”¯æŒå¯é…ç½®çš„é‡è¯•
- **åµŒå…¥å‘é‡**: æ–‡æœ¬åµŒå…¥ç”Ÿæˆï¼Œç”¨äºè¯­ä¹‰æœç´¢å’Œ RAG åº”ç”¨
- **é…ç½®é©±åŠ¨**: åŸºäº YAML çš„é…ç½®ï¼Œä¾¿äºéƒ¨ç½²ç®¡ç†

### ç”Ÿäº§å°±ç»ª
- **å…¨é¢æ—¥å¿—è®°å½•**: è¯¦ç»†çš„æ—¥å¿—è®°å½•ï¼Œç”¨äºè°ƒè¯•å’Œç›‘æ§
- **é”™è¯¯å¤„ç†**: ä¼˜é›…çš„é”™è¯¯å¤„ç†ï¼ŒåŒ…å«è‡ªå®šä¹‰å¼‚å¸¸ç±»å‹  
- **æ€§èƒ½ä¼˜åŒ–**: é«˜æ•ˆçš„èµ„æºä½¿ç”¨å’Œå“åº”ç¼“å­˜
- **çº¿ç¨‹å®‰å…¨**: æ”¯æŒå¤šçº¿ç¨‹åº”ç”¨ä¸­çš„å¹¶å‘ä½¿ç”¨

## ğŸ“¦ å®‰è£…

æ·»åŠ åˆ°ä½ çš„ Gemfileï¼š

```ruby
gem 'smart_prompt'
```

ç„¶åæ‰§è¡Œï¼š
```bash
$ bundle install
```

æˆ–ç›´æ¥å®‰è£…ï¼š
```bash
$ gem install smart_prompt
```

## ğŸ› ï¸ å¿«é€Ÿå¼€å§‹

### 1. é…ç½®

åˆ›å»º YAML é…ç½®æ–‡ä»¶ï¼ˆ`config/smart_prompt.yml`ï¼‰ï¼š

```yaml
# é€‚é…å™¨å®šä¹‰
adapters:
  openai: OpenAIAdapter
# LLM é…ç½®
llms:
  SiliconFlow:
    adapter: openai
    url: https://api.siliconflow.cn/v1/
    api_key: ENV["APIKey"]
    default_model: Qwen/Qwen2.5-7B-Instruct
  llamacpp:
    adapter: openai
    url: http://localhost:8080/    
  ollama:
    adapter: openai
    url: http://localhost:11434/
    default_model: deepseek-r1
  deepseek:
    adapter: openai
    url: https://api.deepseek.com
    api_key: ENV["DSKEY"]
    default_model: deepseek-reasoner

# é»˜è®¤è®¾ç½®
default_llm: SiliconFlow
template_path: "./templates"
worker_path: "./workers"
logger_file: "./logs/smart_prompt.log"
```

### 2. åˆ›å»ºæç¤ºè¯æ¨¡æ¿

åœ¨ `templates/` ç›®å½•ä¸­åˆ›å»ºæ¨¡æ¿æ–‡ä»¶ï¼š

**templates/chat.erb**:
```erb
ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

é—®é¢˜ï¼š<%= question %>

èƒŒæ™¯ï¼š<%= context || "æœªæä¾›é¢å¤–èƒŒæ™¯ä¿¡æ¯" %>
```

### 3. å®šä¹‰ Worker

åœ¨ `workers/` ç›®å½•ä¸­åˆ›å»º worker æ–‡ä»¶ï¼š

**workers/chat_worker.rb**:
```ruby
SmartPrompt.define_worker :chat_assistant do
  # ä½¿ç”¨ç‰¹å®šçš„ LLM
  use "SiliconFlow"
  model "deepseek-ai/DeepSeek-V3"
  # è®¾ç½®ç³»ç»Ÿæ¶ˆæ¯
  sys_msg("ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„ AI åŠ©æ‰‹ã€‚", params)
  # ä½¿ç”¨æ¨¡æ¿å’Œå‚æ•°  
  prompt(:chat, {
    question: params[:question],
    context: params[:context]
  })
  # å‘é€æ¶ˆæ¯å¹¶è¿”å›å“åº”
  send_msg
end
```

### 4. åœ¨åº”ç”¨ä¸­ä½¿ç”¨

```ruby
require 'smart_prompt'

# ä½¿ç”¨é…ç½®åˆå§‹åŒ–å¼•æ“
engine = SmartPrompt::Engine.new('config/smart_prompt.yml')

# æ‰§è¡Œ worker
result = engine.call_worker(:chat_assistant, {
  question: "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
  context: "æˆ‘ä»¬æ­£åœ¨è®¨è®º AI æŠ€æœ¯"
})

puts result
```

## ğŸ“š é«˜çº§ç”¨æ³•

### æµå¼å“åº”

```ruby
# å®šä¹‰æµå¼ worker
SmartPrompt.define_worker :streaming_chat do
  use "deepseek"
  model "deepseek-chat"
  sys_msg("ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚")
  prompt(params[:message])
  send_msg
end

# ä½¿ç”¨æµå¼å¤„ç†
engine.call_worker_by_stream(:streaming_chat, {
  message: "ç»™æˆ‘è®²ä¸ªæ•…äº‹"
}) do |chunk, bytesize|
  print chunk.dig("choices", 0, "delta", "content")
end
```

### å·¥å…·é›†æˆ

```ruby
# å®šä¹‰å¸¦å·¥å…·çš„ worker
SmartPrompt.define_worker :assistant_with_tools do
  use "SiliconFlow"
  model "Qwen/Qwen3-235B-A22B"
  tools = [
    {
      type: "function",
      function: {
        name: "get_weather",
        description: "è·å–æŒ‡å®šä½ç½®çš„å¤©æ°”ä¿¡æ¯",
        parameters: {
          type: "object",
          properties: {
            location: {
              type: "string", 
              description: "åŸå¸‚å’Œçœä»½"
            }
          },
          required: ["location"]
        }
      }
    }
  ]
  
  sys_msg("ä½ å¯ä»¥ä½¿ç”¨å¯ç”¨çš„å·¥å…·å¸®åŠ©å¤„ç†å¤©æ°”æŸ¥è¯¢ã€‚", params)
  prompt(params[:message])
  params.merge(tools: tools)
  send_msg
end
```

### å¯¹è¯å†å²

```ruby
SmartPrompt.define_worker :conversational_chat do
  use "deepseek"
  model "deepseek-chat"
  sys_msg("ä½ æ˜¯ä¸€ä¸ªè®°ä½å¯¹è¯ä¸Šä¸‹æ–‡çš„æœ‰ç”¨åŠ©æ‰‹ã€‚")
  prompt(params[:message], with_history: true)
  send_msg
end
```

### åµŒå…¥å‘é‡ç”Ÿæˆ

```ruby
SmartPrompt.define_worker :text_embedder do
  use "SiliconFlow"
  model "BAAI/bge-m3"
  prompt params[:text]  
  embeddings(params[:dimensions] || 1024)
end

# ä½¿ç”¨æ–¹æ³•
embeddings = engine.call_worker(:text_embedder, {
  text: "å°†æ­¤æ–‡æœ¬è½¬æ¢ä¸ºåµŒå…¥å‘é‡",
  dimensions: 1024
})
```

## ğŸ—ï¸ æ¶æ„æ¦‚è¿°

SmartPrompt é‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     åº”ç”¨ç¨‹åº     â”‚    â”‚   SmartPrompt    â”‚    â”‚   LLM æä¾›å•†    â”‚
â”‚                 â”‚â—„â”€â”€â–ºâ”‚      å¼•æ“        â”‚â—„â”€â”€â–ºâ”‚   (OpenAI/      â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚    Llama.cpp)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚        â”‚        â”‚
                   â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
                   â”‚Worker â”‚ â”‚å¯¹è¯ â”‚ â”‚ æ¨¡æ¿   â”‚
                   â”‚       â”‚ â”‚ç®¡ç† â”‚ â”‚ ç³»ç»Ÿ   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒç»„ä»¶

- **å¼•æ“ï¼ˆEngineï¼‰**: ä¸­å¤®ç¼–æ’å™¨ï¼Œç®¡ç†é…ç½®ã€é€‚é…å™¨å’Œ Worker
- **Worker**: åŒ…å«åµŒå…¥ä¸šåŠ¡é€»è¾‘çš„å¯é‡ç”¨ä»»åŠ¡å®šä¹‰
- **å¯¹è¯ï¼ˆConversationï¼‰**: ä¸Šä¸‹æ–‡å’Œæ¶ˆæ¯å†å²ç®¡ç†
- **é€‚é…å™¨ï¼ˆAdaptersï¼‰**: LLM æä¾›å•†é›†æˆï¼ˆOpenAIã€Llama.cpp ç­‰ï¼‰
- **æ¨¡æ¿ï¼ˆTemplatesï¼‰**: åŸºäº ERB çš„æç¤ºè¯æ¨¡æ¿ç³»ç»Ÿ

## ğŸ”§ é…ç½®å‚è€ƒ

### é€‚é…å™¨é…ç½®

```yaml
adapters:
  openai: "OpenAIAdapter"      # ç”¨äº OpenAI API
```

### LLM é…ç½®

```yaml
llms:
  model_name:
    adapter: "adapter_name"
    api_key: "your_api_key"     # å¯ä»¥ä½¿ç”¨ ENV['KEY_NAME']
    url: "https://api.url"
    model: "model_identifier"
    temperature: 0.7
    # å…¶ä»–æä¾›å•†ç‰¹å®šé€‰é¡¹
```

### è·¯å¾„é…ç½®

```yaml
template_path: "./templates"   # .erb æ¨¡æ¿ç›®å½•
worker_path: "./workers"       # worker å®šä¹‰ç›®å½•  
logger_file: "./logs/app.log"  # æ—¥å¿—æ–‡ä»¶ä½ç½®
```

## ğŸ§ª æµ‹è¯•

è¿è¡Œæµ‹è¯•å¥—ä»¶ï¼š

```bash
bundle exec rake test
```

å¼€å‘æ—¶ï¼Œå¯ä»¥ä½¿ç”¨æ§åˆ¶å°ï¼š

```bash
bundle exec bin/console
```

## ğŸ¤ é›†æˆç¤ºä¾‹

### ä¸ Rails åº”ç”¨é›†æˆ

```ruby
# config/initializers/smart_prompt.rb
class SmartPromptService
  def self.engine
    @engine ||= SmartPrompt::Engine.new(
      Rails.root.join('config', 'smart_prompt.yml')
    )
  end
  
  def self.chat(message, context: nil)
    engine.call_worker(:chat_assistant, {
      question: message,
      context: context
    })
  end
end

# åœ¨æ§åˆ¶å™¨ä¸­ä½¿ç”¨
class ChatController < ApplicationController
  def create
    response = SmartPromptService.chat(
      params[:message],
      context: session[:conversation_context]
    )
    
    render json: { response: response }
  end
end
```

### ä¸ Sidekiq åå°ä»»åŠ¡é›†æˆ

```ruby
class LLMProcessingJob < ApplicationJob
  def perform(task_type, parameters)
    engine = SmartPrompt::Engine.new('config/smart_prompt.yml')
    result = engine.call_worker(task_type.to_sym, parameters)
    
    # å¤„ç†ç»“æœ...
    NotificationService.send_completion(result)
  end
end
```

## ğŸš€ å®é™…åº”ç”¨åœºæ™¯

- **èŠå¤©æœºå™¨äººå’Œå¯¹è¯å¼ AI**: æ„å»ºå…·æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„å¤æ‚èŠå¤©æœºå™¨äºº
- **å†…å®¹ç”Ÿæˆ**: åŸºäºæ¨¡æ¿é©±åŠ¨çš„æç¤ºè¯è¿›è¡Œè‡ªåŠ¨åŒ–å†…å®¹åˆ›å»º  
- **ä»£ç åˆ†æ**: AI é©±åŠ¨çš„ä»£ç å®¡æŸ¥å’Œæ–‡æ¡£ç”Ÿæˆ
- **å®¢æˆ·æ”¯æŒ**: æ™ºèƒ½å·¥å•è·¯ç”±å’Œå“åº”å»ºè®®
- **æ•°æ®å¤„ç†**: LLM é©±åŠ¨çš„æ•°æ®æå–å’Œè½¬æ¢
- **æ•™è‚²å·¥å…·**: AI å¯¼å¸ˆå’Œå­¦ä¹ è¾…åŠ©ç³»ç»Ÿ

## ğŸ›£ï¸ å‘å±•è·¯çº¿å›¾

- [ ] æ–°å¢ LLM æä¾›å•†é€‚é…å™¨ï¼ˆAnthropic Claudeã€Google PaLMï¼‰
- [ ] å¯è§†åŒ–æç¤ºè¯æ„å»ºå™¨å’Œç®¡ç†ç•Œé¢
- [ ] å¢å¼ºç¼“å­˜å’Œæ€§èƒ½ä¼˜åŒ–
- [ ] ä¸å‘é‡æ•°æ®åº“é›†æˆï¼Œæ”¯æŒ RAG åº”ç”¨
- [ ] å†…ç½®æç¤ºè¯è¯„ä¼°å’Œæµ‹è¯•æ¡†æ¶
- [ ] åˆ†å¸ƒå¼ worker æ‰§è¡Œæ”¯æŒ

## ğŸ¤ è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿è´¡çŒ®ï¼è¯·æŸ¥çœ‹ [CONTRIBUTING.md](CONTRIBUTING.md) äº†è§£æŒ‡å—ã€‚

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºä½ çš„åŠŸèƒ½åˆ†æ”¯ï¼ˆ`git checkout -b feature/amazing-feature`ï¼‰
3. æäº¤ä½ çš„æ›´æ”¹ï¼ˆ`git commit -am 'Add amazing feature'`ï¼‰
4. æ¨é€åˆ°åˆ†æ”¯ï¼ˆ`git push origin feature/amazing-feature`ï¼‰
5. å¼€å¯ä¸€ä¸ª Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®ä½¿ç”¨ MIT è®¸å¯è¯ - è¯¦æƒ…è¯·æŸ¥çœ‹ [LICENSE.txt](LICENSE.txt) æ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

- ç”± SmartPrompt å›¢é˜Ÿç”¨ â¤ï¸ æ„å»º
- å—åˆ°åœ¨ Ruby åº”ç”¨ä¸­ä¼˜é›…é›†æˆ LLM éœ€æ±‚çš„å¯å‘
- æ„Ÿè°¢æ‰€æœ‰è´¡çŒ®è€…å’Œ Ruby ç¤¾åŒº

## ğŸ“ æ”¯æŒ

- ğŸ“– [æ–‡æ¡£](https://github.com/zhuangbiaowei/smart_prompt/wiki)
- ğŸ› [é—®é¢˜è¿½è¸ª](https://github.com/zhuangbiaowei/smart_prompt/issues)
- ğŸ’¬ [è®¨è®ºåŒº](https://github.com/zhuangbiaowei/smart_prompt/discussions)
- ğŸ“§ é‚®ç®±ï¼šzbw@kaiyuanshe.org

---

**SmartPrompt** - è®© Ruby åº”ç”¨ä¸­çš„ LLM é›†æˆå˜å¾—ç®€å•ã€å¼ºå¤§ä¸”ä¼˜é›…ã€‚